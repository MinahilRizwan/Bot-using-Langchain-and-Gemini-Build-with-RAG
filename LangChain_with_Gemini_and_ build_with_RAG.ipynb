{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kishore8949/LLMsApplications/blob/main/Langchain_with_Gemini_and_Build_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E0joGue2sfm",
        "outputId": "ccb4e643-6d23-45a5-98de-be8f9ede3df4"
      },
      "outputs": [],
      "source": [
        "! pip install -q --upgrade google-generativeai langchain-google-genai chromadb pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8IGZrFZ_SVA"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSMPfCFyAYP1"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"YOUR_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import google.generativeai as genai\n",
        "\n",
        "load_dotenv()  # Load from .env\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE6Qwi77IvZb"
      },
      "source": [
        "# Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1XGUGrfIxRA",
        "outputId": "c02f16c1-f369-4ea9-f980-6127bdcd73f2"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(model_name = \"gemini-2.0-flash\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xHy-OzbI42Z"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your API Key here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "1zuu1y9NKCi1",
        "outputId": "70ddbb9f-fe36-413f-d6b5-a6df6e16a6aa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "#  Set your real API key here\n",
        "GOOGLE_API_KEY = \"Your API Key here\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "#  Configure the SDK\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Load the Gemini Pro model\n",
        "model = genai.GenerativeModel(model_name=\"models/gemini-2.0-flash\")\n",
        "\n",
        "#  Generate content\n",
        "try:\n",
        "    response = model.generate_content(\"What are the use cases of LLMs?\")\n",
        "\n",
        "    # Print response using .text (preferred in SDK v0.4+)\n",
        "    print(response.text)\n",
        "\n",
        "except AttributeError:\n",
        "    # Fallback for older versions or unexpected structures\n",
        "    print(response.candidates[0].content.parts[0].text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"❌ Error occurred:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7DzltSTI_lG"
      },
      "outputs": [],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z3fk78uJQJu"
      },
      "source": [
        "# Using langchain to access Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUdgRnkFJNVL"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUippAifJZ2b"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash\",google_api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -U langchain langchain-google-genai google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "OAl-u6O6JiOq",
        "outputId": "ae376bfe-5780-475c-e22e-33569fba678f"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "import os\n",
        "\n",
        "# Set your real Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your API Key here\"\n",
        "\n",
        "# Initialize the Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"models/gemini-2.0-flash\",  # Ensure this model is available for your API key\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
        ")\n",
        "\n",
        "# Call 'invoke' method with HumanMessage\n",
        "result = llm.invoke([HumanMessage(content=\"what are the use cases of LLM\")])\n",
        "\n",
        "# Print the result\n",
        "print(result.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NHgLN1AJiMR"
      },
      "outputs": [],
      "source": [
        "to_markdown(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dofj1m3xJiJk"
      },
      "outputs": [],
      "source": [
        "from PIL import Image  \n",
        "img = Image.open('safety gear.jpg')\n",
        "img.show()  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSBSXQt4JiE1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "# Set your actual API key\n",
        "GOOGLE_API_KEY = \"Your API Key here\"\n",
        "\n",
        "# Initialize the Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"models/gemini-2.0-flash\", \n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Correct image URL (replace with actual direct image URL)\n",
        "image_url = \"https://images.unsplash.com/photo-1513122503469-4197cc9c4ff3\"  \n",
        "\n",
        "# Create the message content\n",
        "message_content = f\"\"\"\n",
        "Write a short description about the product shown in the image for mentioning it on a construction website.\n",
        "Image URL: {image_url}\n",
        "\"\"\"\n",
        "\n",
        "# Create the HumanMessage object\n",
        "message = HumanMessage(content=message_content)\n",
        "\n",
        "# Send the message to Gemini for processing\n",
        "response = llm.invoke([message])\n",
        "\n",
        "# Print the response\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LazHBQ05K0GQ"
      },
      "source": [
        "# Chat with Documents using RAG (Retreival Augment Generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiOaaYIUJh91"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
        "\n",
        "!sudo apt-get -y -qq install poppler-utils libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzhx4EBOK_lr"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# restart python kernal if issues with langchain import."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_x36wdLK_h6"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81M4TK1rK_gN"
      },
      "outputs": [],
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,\n",
        "                             temperature=0.2,convert_system_message_to_human=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViZEJm52LzQz"
      },
      "source": [
        "# Extract text from PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "5AHwwlqmK_eF",
        "outputId": "46055f99-e747-4a50-e0a8-92ead74be651"
      },
      "outputs": [],
      "source": [
        "pdf_loader = PyPDFLoader(\"NIPS-2017-attention-is-all-you-need-Paper.pdf\")\n",
        "pages = pdf_loader.load_and_split()\n",
        "print(pages[3].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt4o4qwEK_b-"
      },
      "outputs": [],
      "source": [
        "len(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhoY-2GlMQW2"
      },
      "source": [
        "# RAG Pipeline: Embedding + Gemini (LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ7QrabgM01b"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tswPTvchK_Zu"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
        "texts = text_splitter.split_text(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyuPx7woMoHr"
      },
      "outputs": [],
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKWeNmStNIXu"
      },
      "outputs": [],
      "source": [
        "vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjVScVbWNTYb"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vector_index,\n",
        "    return_source_documents=True\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install langchain-google-genai langchain chromadb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl9lt-IsNcZM"
      },
      "outputs": [],
      "source": [
        "splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embedding)\n",
        "retriever = vectorstore.as_retriever()\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "result = qa_chain({\"query\": \"Describe the Multi-head attention layer in detail?\"})\n",
        "print(result[\"result\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDoRftVyNktB"
      },
      "outputs": [],
      "source": [
        "Markdown(result[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(result[\"result\"])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF-AMyl0OLH1"
      },
      "outputs": [],
      "source": [
        "question = \"Describe Random forest?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "Markdown(result[\"result\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
